{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'crawl-300d-2M.vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3be5e61ca0d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mvecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_vectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'crawl-300d-2M.vec'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-3be5e61ca0d1>\u001b[0m in \u001b[0;36mload_vectors\u001b[1;34m(fname, limit)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_vectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m   \u001b[0mfin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m   \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m   \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'crawl-300d-2M.vec'"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "\n",
    "def load_vectors(fname, limit):\n",
    "  fin = io.open(fname, 'r', encoding = 'utf-8', newline = '\\n', errors = 'ignore')\n",
    "  n, d = map(int, fin.readline().split())\n",
    "  data = {}\n",
    "  for line in tqdm(islice(fin, limit), total = limit):\n",
    "    tokens = line.rstrip().split(' ')\n",
    "    data[tokens[0]] = np.array(list(map(float, tokens[1:])))\n",
    "  return data\n",
    "\n",
    "vecs = load_vectors('crawl-300d-2M.vec', 100000)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_nearest_neighbors(vec, k):\n",
    "  return list(zip(*sorted(list(map(lambda key: (np.linalg.norm(vec - vecs[key]), key), vecs.keys())))))[1][:k]\n",
    "\n",
    "print(get_k_nearest_neighbors(vecs['Paris'], 20))\n",
    "print(get_k_nearest_neighbors(vecs['brother'], 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_k_nearest_neighbors(vecs['Paris'] - vecs['France'] + vecs['Germany'], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_k_nearest_neighbors(vecs['brother'] - vecs['man'] + vecs['woman'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_k_nearest_neighbors(vecs['king'] - vecs['man'] + vecs['woman'], 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset = 'train', categories = categories, shuffle = True, random_state = 42)\n",
    "twenty_test = fetch_20newsgroups(subset='test', categories = categories,shuffle = True, random_state = 42)\n",
    "X_train = twenty_train.data\n",
    "y_train = twenty_train.target\n",
    "X_test = twenty_test.data\n",
    "y_test = twenty_test.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = load_vectors('crawl-300d-2M.vec', 2000000)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "logreg = LogisticRegression(solver = 'liblinear', multi_class = 'ovr', random_state = 1)\n",
    "\n",
    "logreg.fit(X_train_tfidf, y_train)\n",
    "y_pred = logreg.predict(X_test_tfidf)\n",
    "print(accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 300, random_state = 1)\n",
    "X_train_tfidf_pca = pca.fit_transform(X_train_tfidf.todense())\n",
    "X_test_tfidf_pca = pca.transform(X_test_tfidf.todense())\n",
    "\n",
    "logreg.fit(X_train_tfidf_pca, y_train)\n",
    "y_pred_pca = logreg.predict(X_test_tfidf_pca)\n",
    "print(accuracy_score(y_test, y_pred_pca))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "zero = sum(vecs.values()) / len(vecs)\n",
    "def text2vec(text):\n",
    "  words = text.split()\n",
    "  return sum(list(map(lambda w: np.array(list(vecs.get(w, zero))), words))) / len(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec = list(map(lambda text: text2vec(text), X_train))\n",
    "X_test_vec = list(map(lambda text: text2vec(text), X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train_vec, y_train)\n",
    "y_pred_vec = logreg.predict(X_test_vec)\n",
    "print(accuracy_score(y_test, y_pred_vec))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 300\n",
    "vocab = np.zeros((len(tfidf.vocabulary_.keys()), dim))\n",
    "for key in tqdm(tfidf.vocabulary_.keys()):\n",
    "  vocab[tfidf.vocabulary_[key]] = vecs.get(key, zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_weighted = X_train_tfidf.dot(vocab)\n",
    "X_test_weighted = X_test_tfidf.dot(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train_weighted, y_train)\n",
    "y_pred_weighted = logreg.predict(X_test_weighted)\n",
    "print(accuracy_score(y_test, y_pred_weighted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "reviews = pd.read_csv('imdb_master.csv', encoding = 'latin-1')['review']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = list(map(lambda text: text.split(), reviews))\n",
    "model = Word2Vec(sentences, min_count = 1, seed = 1, workers = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = model.wv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(wv.similar_by_word('Paris'))\n",
    "print()\n",
    "print(wv.similar_by_word('brother'))\n",
    "print()\n",
    "print(wv.similar_by_vector(wv['king'] - wv['man'] + wv['woman'], 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
